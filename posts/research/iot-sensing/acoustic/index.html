<!DOCTYPE html>
<html lang="en">
  <head>
    <title>Acoustic and imagery sensing</title>
    <meta charset="UTF-8" />
<meta name="viewport" content="width=device-width, initial-scale=1.0" />
<meta http-equiv="X-UA-Compatible" content="ie=edge" />

<link rel="stylesheet" href="/application.d77e8171484c13bd478e107100bebb7c3abfe57e9546da67c9f850907b248921.css" integrity="sha256-136BcUhME71HjhBxAL67fDq/5X6VRtpnyfhQkHskiSE=" />





  

  
  
  
    
  
  

  <link rel="icon" type="image/png" href="/images/site/logo_hu7f8513698206be1b76723ad612f02b43_196744_42x0_resize_box_3.png" />

<meta property="og:title" content="Acoustic and imagery sensing" />
<meta property="og:description" content="Main contributors from the group: Wenjie Luo (topic coordinator), Qun Song, Chaojie Gu, Zhenyu Yan, Duc Van Le, Siyuan Zhou, Jiale Chen, Qiping (Joy) Yang
The acoustic and imagery sensing modalities provide abundant information about the sensed target. As a result, the volume of an acoustic/imagery signal sample is often high and the data processing to extract useful information requires intensive computing. Such characteristics introduce various system challenges in data acquisition, computing, storage, and network transmission on networked sensing platforms with constrained processing capabilities, network bandwidth, and bounded energy supply." />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://rrwang1.github.io/posts/research/iot-sensing/acoustic/" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2023-07-01T00:00:00+00:00" />
<meta property="article:modified_time" content="2023-07-01T00:00:00+00:00" />


    
    
<meta name="description" content="Acoustic and imagery sensing" />


    

    




</head>

  <body class="type-posts kind-page" data-spy="scroll" data-target="#TableOfContents" data-offset="80">
    <div class="container-fluid bg-dimmed wrapper">
      
      
    





  


  




  
  
    
  
  



  
  
    
  
  


<nav class="navbar navbar-expand-xl top-navbar final-navbar shadow">
  <div class="container">
      <button class="navbar-toggler navbar-light" id="sidebar-toggler" type="button">
      <span class="navbar-toggler-icon"></span>
    </button>
    <a class="navbar-brand" href="/">
      
        <img src="/images/site/logo_hu7f8513698206be1b76723ad612f02b43_196744_42x0_resize_box_3.png" alt="Logo">
      NTU IoT Research Group</a>
    <button class="navbar-toggler navbar-light" id="toc-toggler" type="button">
      <span class="navbar-toggler-icon"></span>
    </button>

    <div class="collapse navbar-collapse lang-selector" id="top-nav-items">
      <ul class="navbar-nav ml-auto">
      
      
        <li class="nav-item dropdown">
<a class="nav-link dropdown-toggle"  href="#" id="themeSelector" role="button"
  data-toggle="dropdown" aria-haspopup="true" aria-expanded="false">
  <img id="navbar-theme-icon-svg" src="/icons/moon-svgrepo-com.svg" width=20 alt="Dark Theme">
</a>
<div id="themeMenu" class="dropdown-menu dropdown-menu-icons-only" aria-labelledby="themeSelector">
  <a class="dropdown-item nav-link" href="#" data-scheme="light">
    <img class="menu-icon-center" src="/icons/sun-svgrepo-com.svg" width=20 alt="Light Theme">
  </a>
  <a class="dropdown-item nav-link" href="#" data-scheme="dark">
    <img class="menu-icon-center" src="/icons/moon-svgrepo-com.svg" width=20 alt="Dark Theme">
  </a>
  <a class="dropdown-item nav-link" href="#" data-scheme="system">
    <img class="menu-icon-center" src="/icons/computer-svgrepo-com.svg" width=20 alt="System Theme">
  </a>
</div>
</li>

      
      </ul>
    </div>
  </div>
  
  
    <img src="/images/site/logo_hu7f8513698206be1b76723ad612f02b43_196744_42x0_resize_box_3.png" class="d-none" id="main-logo" alt="Logo">
  
  
    <img src="/images/site/logo_hu7f8513698206be1b76723ad612f02b43_196744_42x0_resize_box_3.png" class="d-none" id="inverted-logo" alt="Inverted Logo">
  
</nav>



      
      
  <section class="sidebar-section" id="sidebar-section">
    <div class="sidebar-holder">
      <div class="sidebar" id="sidebar">
        <form class="mx-auto" method="get" action="/search">
          <input type="text" name="keyword" value="" placeholder="Search" data-search="" id="search-box" />
        </form>
        <div class="sidebar-tree">
          <ul class="tree" id="tree">
            <li id="list-heading"><a href="/posts/" data-filter="all">Posts</a></li>
            <div class="subtree">
                
  
  
  
  
    
    
  
  
    
    <li>
      <i class="fas fa-minus-circle"></i><a class="active" href="/posts/research/">1. Research</a>
      
      <ul class="active">
        
  
  
  
  
  
    
    <li><a class="" href="/posts/research/intro/" title="Research Focus">Research Focus</a></li>
  

  
  
  
  
  
    
    <li>
      <i class="fas fa-plus-circle"></i><a class="" href="/posts/research/security/">Security and privacy of AIoT sensing</a>
      
      <ul class="">
        
  
  
  
  
  
    
    <li><a class="" href="/posts/research/security/secure/" title="Secure sensing in AIoT">Secure sensing in AIoT</a></li>
  

  
  
  
  
  
    
    <li><a class="" href="/posts/research/security/privicy/" title="Privacy-preserving sensing in AIoT">Privacy-preserving sensing in AIoT</a></li>
  


      </ul>
    </li>
  

  
  
  
  
    
    
  
  
    
    <li>
      <i class="fas fa-minus-circle"></i><a class="active" href="/posts/research/iot-sensing/">IoT sensing systems and applications</a>
      
      <ul class="active">
        
  
  
  
  
  
    
    <li><a class="" href="/posts/research/iot-sensing/powerline/" title="Powerline radiation sensing">Powerline radiation sensing</a></li>
  

  
  
  
  
  
    
    <li><a class="" href="/posts/research/iot-sensing/lora/" title="LoRa radio sensing">LoRa radio sensing</a></li>
  

  
  
  
  
  
    
    <li><a class="" href="/posts/research/iot-sensing/building/" title="Building sensing">Building sensing</a></li>
  

  
  
  
  
    
    
  
  
    
    <li><a class="active" href="/posts/research/iot-sensing/acoustic/" title="Acoustic and imagery sensing">Acoustic and imagery sensing</a></li>
  


      </ul>
    </li>
  


      </ul>
    </li>
  

  
  
  
  
  
    
    <li>
      <i class="fas fa-plus-circle"></i><a class="" href="/posts/members/">2. Members</a>
      
      <ul class="">
        
  
  
  
  
  
    
    <li><a class="" href="/posts/members/faculty/" title="Faculty">Faculty</a></li>
  

  
  
  
  
  
    
    <li><a class="" href="/posts/members/alumni/" title="Alumni">Alumni</a></li>
  

  
  
  
  
  
    
    <li>
      <i class="fas fa-plus-circle"></i><a class="" href="/posts/members/phd/">PhD Candidates</a>
      
      <ul class="">
        
  
  
  
  
  
    
    <li><a class="" href="/posts/members/phd/yimin/" title="Yimin Dai">Yimin Dai</a></li>
  

  
  
  
  
  
    
    <li><a class="" href="/posts/members/phd/wenjie/" title="Wenjie Luo">Wenjie Luo</a></li>
  

  
  
  
  
  
    
    <li><a class="" href="/posts/members/phd/dongfang/" title="Dongfang Guo">Dongfang Guo</a></li>
  

  
  
  
  
  
    
    <li><a class="" href="/posts/members/phd/jiale/" title="Jiale Chen">Jiale Chen</a></li>
  

  
  
  
  
  
    
    <li><a class="" href="/posts/members/phd/rongrong/" title="Rongrong Wang">Rongrong Wang</a></li>
  

  
  
  
  
  
    
    <li><a class="" href="/posts/members/phd/siyuan/" title="Siyuan Zhou">Siyuan Zhou</a></li>
  


      </ul>
    </li>
  

  
  
  
  
  
    
    <li>
      <i class="fas fa-plus-circle"></i><a class="" href="/posts/members/visit/">Visiting Students</a>
      
      <ul class="">
        
  
  
  
  
  
    
    <li><a class="" href="/posts/members/visit/huimin/" title="Huimin Chen">Huimin Chen</a></li>
  

  
  
  
  
  
    
    <li><a class="" href="/posts/members/visit/lilin/" title="Lilin Xu">Lilin Xu</a></li>
  


      </ul>
    </li>
  


      </ul>
    </li>
  

  
  
  
  
  
    
    <li>
      <i class="fas fa-plus-circle"></i><a class="" href="/posts/call/">3. Call for Undergrads</a>
      
      <ul class="">
        
  
  
  
  
  
    
    <li><a class="" href="/posts/call/achievements/" title="Undergrad achievements">Undergrad achievements</a></li>
  

  
  
  
  
  
    
    <li><a class="" href="/posts/call/opportunity/" title="Opportunities for new undergrads">Opportunities for new undergrads</a></li>
  


      </ul>
    </li>
  

  
  
  
  
  
    
    <li>
      <i class="fas fa-plus-circle"></i><a class="" href="/posts/blog/">4. Blog</a>
      
      <ul class="">
        
  
  
  
  
  
    
    <li>
      <i class="fas fa-plus-circle"></i><a class="" href="/posts/blog/conference/">Conference</a>
      
      <ul class="">
        
  
  
  
  
  
    
    <li><a class="" href="/posts/blog/conference/aichallengeiot-2019/" title="AIChallengeIoT 2019">AIChallengeIoT 2019</a></li>
  

  
  
  
  
  
    
    <li><a class="" href="/posts/blog/conference/cps-iot-2023/" title="CPS-IoT 2023">CPS-IoT 2023</a></li>
  

  
  
  
  
  
    
    <li><a class="" href="/posts/blog/conference/e-energy-2021-secon-2021/" title="e-Energy 2021 - SECON 2021">e-Energy 2021 - SECON 2021</a></li>
  

  
  
  
  
  
    
    <li><a class="" href="/posts/blog/conference/e-energy-2023/" title="e-Energy 2023">e-Energy 2023</a></li>
  

  
  
  
  
  
    
    <li><a class="" href="/posts/blog/conference/embedded-al-2019/" title="Embedded AI Submit 2019">Embedded AI Submit 2019</a></li>
  

  
  
  
  
  
    
    <li><a class="" href="/posts/blog/conference/ewsn-2019/" title="EWSN 2019">EWSN 2019</a></li>
  

  
  
  
  
  
    
    <li><a class="" href="/posts/blog/conference/fog-computing/" title="Fog Computing&#43; Workshop">Fog Computing&#43; Workshop</a></li>
  

  
  
  
  
  
    
    <li><a class="" href="/posts/blog/conference/ieee-pes-day-2021/" title="IEEE PES Day 2021">IEEE PES Day 2021</a></li>
  

  
  
  
  
  
    
    <li><a class="" href="/posts/blog/conference/iotdi-2019/" title="IoTDI 2019">IoTDI 2019</a></li>
  

  
  
  
  
  
    
    <li><a class="" href="/posts/blog/conference/ipsn-2021/" title="IPSN 2021">IPSN 2021</a></li>
  

  
  
  
  
  
    
    <li><a class="" href="/posts/blog/conference/ipsn-tpc-2020/" title="IPSN TPC 2020">IPSN TPC 2020</a></li>
  

  
  
  
  
  
    
    <li><a class="" href="/posts/blog/conference/isorc-2018/" title="ISORC 2018">ISORC 2018</a></li>
  

  
  
  
  
  
    
    <li><a class="" href="/posts/blog/conference/mobicom-2019/" title="MobiCom 2019">MobiCom 2019</a></li>
  

  
  
  
  
  
    
    <li><a class="" href="/posts/blog/conference/ngcps-2018/" title="NGCPS 2018">NGCPS 2018</a></li>
  

  
  
  
  
  
    
    <li><a class="" href="/posts/blog/conference/sensys-2018/" title="SenSys 2018">SenSys 2018</a></li>
  

  
  
  
  
  
    
    <li><a class="" href="/posts/blog/conference/sensys-2019/" title="SenSys 2019">SenSys 2019</a></li>
  

  
  
  
  
  
    
    <li><a class="" href="/posts/blog/conference/sensys-2022/" title="SenSys 2022">SenSys 2022</a></li>
  

  
  
  
  
  
    
    <li><a class="" href="/posts/blog/conference/ubicomp-2018/" title="UobiComp 2018">UobiComp 2018</a></li>
  


      </ul>
    </li>
  

  
  
  
  
  
    
    <li>
      <i class="fas fa-plus-circle"></i><a class="" href="/posts/blog/event/">Event</a>
      
      <ul class="">
        
  
  
  
  
  
    
    <li><a class="" href="/posts/blog/event/chaojie-phd-def/" title="Chaojie PhD defense 2020">Chaojie PhD defense 2020</a></li>
  

  
  
  
  
  
    
    <li><a class="" href="/posts/blog/event/career-zc/" title="CUHK and ZJU Faculty">CUHK and ZJU Faculty</a></li>
  

  
  
  
  
  
    
    <li><a class="" href="/posts/blog/event/hawaii-2018/" title="Hawaii 2018">Hawaii 2018</a></li>
  

  
  
  
  
  
    
    <li><a class="" href="/posts/blog/event/linshan-phd-def/" title="Linshan PhD defense 2021">Linshan PhD defense 2021</a></li>
  

  
  
  
  
  
    
    <li><a class="" href="/posts/blog/event/los-cabos-2019/" title="Los Cabos 2019">Los Cabos 2019</a></li>
  

  
  
  
  
  
    
    <li><a class="" href="/posts/blog/event/montreal-2019/" title="Montreal 2019">Montreal 2019</a></li>
  

  
  
  
  
  
    
    <li><a class="" href="/posts/blog/event/newyork-2019/" title="New York 2019">New York 2019</a></li>
  

  
  
  
  
  
    
    <li><a class="" href="/posts/blog/event/qun-phd-def/" title="Qun PhD defense 2022">Qun PhD defense 2022</a></li>
  

  
  
  
  
  
    
    <li><a class="" href="/posts/blog/event/career-qun/" title="TU Delft Faculty">TU Delft Faculty</a></li>
  

  
  
  
  
  
    
    <li><a class="" href="/posts/blog/event/wenjie-phd-def/" title="Wenjie PhD defense 2023">Wenjie PhD defense 2023</a></li>
  

  
  
  
  
  
    
    <li><a class="" href="/posts/blog/event/zhenyu-award/" title="Zhenyu Award 2021">Zhenyu Award 2021</a></li>
  

  
  
  
  
  
    
    <li><a class="" href="/posts/blog/event/zhenyu-cov/" title="Zhenyu Convocation 2020">Zhenyu Convocation 2020</a></li>
  

  
  
  
  
  
    
    <li><a class="" href="/posts/blog/event/zhenyu-phd-def/" title="Zhenyu PhD defense 2020">Zhenyu PhD defense 2020</a></li>
  


      </ul>
    </li>
  


      </ul>
    </li>
  


            </div>
          </ul>
        </div>
      </div>
    </div>
  </section>


      
      
<section class="content-section" id="content-section">
  <div class="content">
    <div class="container p-0 read-area">
      
      <div class="hero-area col-sm-12" id="hero-area" style='background-image: url(/images/section/cover.png);'>
      </div>

      
      <div class="page-content">
        <div class="author-profile ml-auto align-self-lg-center">
          <img class="rounded-circle" src='/images/site/logo_hu7f8513698206be1b76723ad612f02b43_196744_120x120_fit_box_3.png' alt="Author Image">
          <h5 class="author-name">NTU IoT Sensing Group</h5>
          <p>Saturday, July 1, 2023</p>
        </div>

        <div class="title">
          <h1>Acoustic and imagery sensing</h1>
        </div>
        
        <div class="taxonomy-terms">
          <ul style="padding-left: 0;">
            
          </ul>
        </div>
        
        <div class="post-content" id="post-content">
          <blockquote>
<p>Main contributors from the group: Wenjie Luo (topic coordinator), Qun Song, Chaojie Gu, Zhenyu Yan, Duc Van Le, Siyuan Zhou, Jiale Chen, Qiping (Joy) Yang</p>
</blockquote>
<div style="margin-top: 3rem;"></div>
<p>The acoustic and imagery sensing modalities provide abundant information about the sensed target. As a result, the volume of an acoustic/imagery signal sample is often high and the data processing to extract useful information requires intensive computing. Such characteristics introduce various system challenges in data acquisition, computing, storage, and network transmission on networked sensing platforms with constrained processing capabilities, network bandwidth, and bounded energy supply.</p>
<p>Dr. Rui Tan’s early research designed efficient systems for 1) networked seismic sensing that introduces similar challenges [1, 2, 3] and 2) robotic fish’s visual sensing for monitoring aquatic debris [4] and harmful aquatic processes (e.g., oil spill and harmful algal blooms) [5]. The group is currently developing an acoustic echolocation system based on deep learning and networked imagery sensing for inspection tasks in manufacturing systems. This page briefly describes a completed preliminary work of using inaudible echos for room recognition.</p>
<h3 id="room-recognition-using-inaudible-echos">Room recognition using inaudible echos</h3>
<p>Location awareness is increasingly needed by mobile applications. As of November 2017, 62% of the top 100 free Android Apps on Google Play require location services. While GPS can provide outdoor locations with satisfactory accuracy, determining indoor locations has been a hard problem. Our work designs a practical room-level localization approach for off-the-shelf smartphones using their built-in audio systems only. Room-level localization is desirable in a range of ubiquitous computing applications. For instance, in a hospital, knowing which room that a patient is in is important to responsive medical aid when the patient develops an emergent condition (e.g., falling in a faint). In a museum, knowing which exhibition chamber that a tourist is in can largely assist the automation of her multimedia guide that is often provided as a mobile App nowadays. In a smart building, the room-level localization of the residents can assist the automation of illumination and air conditioning to improve energy efficiency and occupant comfort.</p>
<div style="margin-top: 3rem;"></div>
<img src="/posts/research/images/sensing/fig_3.png"
    
    
    
    
    
        class="center"
    
>

<div style="margin-top: 3rem;"></div>
<p>In our approach, a smartphone uses its loudspeaker to transmit a 2-millisecond narrowband acoustic signal at a frequency (e.g., 20kHz) beyond human’s hearing limit and uses its microphone to capture the reverberation from the indoor environment (typically, a room) for 100 milliseconds. Based on the spectrogram of the 100-millisecond reverbnation, our approach can recognize the indoor environment via a machine learning algorithm. The short audio recording time (i.e., 100 milliseconds) helps preserve the user’s privacy. However, the environment’s response to such a short-term and band-limited acoustic excitation may contain limited information about the environment. To address this, we employ deep learning to train a convolutional neural network for the room recognition.</p>
<p>We conducted extensive experiments to evaluate our approach. Results show 99.7%, 97.7%, 99%, and 89% accuracy in differentiating 22 and 50 residential/office rooms, 19 spots in a quiet museum, and 15 spots in a crowded museum, respectively. The below figures show several example room types and university tutorial rooms having similar appearance that are employed in our evaluation experiments, as well as the 19 and 15 spots in the two different museums that we experimented with. The results were published on ACM Ubicomp’18 <a href="https://personal.ntu.edu.sg/tanrui/pub/RoomRecognize.pdf" target="_blank" rel="noopener">(PDF)</a>.</p>
<div style="margin-top: 3rem;"></div>
<img src="/posts/research/images/sensing/fig_12.png"
    
    
    
    
    
        class="center"
    
>

<div style="margin-top: 3rem;"></div>
<table>
<thead>
<tr>
<th></th>
<th></th>
<th></th>
<th></th>
</tr>
</thead>
<tbody>
<tr>
<td>    </td>
<td><img src="/posts/research/images/sensing/fig_24.png"
    
    
        width="480px"
    
    
    
    
>
</td>
<td> </td>
<td><img src="/posts/research/images/sensing/fig_25.png"
    
    
        width="200px"
    
    
    
    
>
</td>
</tr>
</tbody>
</table>
<h3 id="imagery-sensing-for-manufacturing-system">Imagery sensing for manufacturing system</h3>
<p>Computer vision has become an essential component of the automated inspection processes in smart manufacturing systems. We design and implement an Edge-Fog Camera (EFCam) system that can adapt its configuration with deep reinforcement learning. EFCam leverages the off-the-shelf battery-powered wireless camera called ESP32-CAM at the edge and fog node (e.g., Raspberry Pi) to achieve cordless and energy-efficient operations in industrial systems. The wireless cameras can offer various benefits such as easy deployment, mobility support, and unobtrusiveness to the ongoing industrial processes. Additionally, wall-powered wireless fog node with sufficient computing resources is used to support the camera in facilitating deep learning (DL)-based image processing with short jitters and delays.</p>
<div style="margin-top: 3rem;"></div>
<img src="/posts/research/images/sensing/fig_efcam.png"
    
    
    
    
    
        class="center"
    
>

<div style="margin-top: 3rem;"></div>
<p>In our EFCam system, the wireless camera performs image pre-processing, and then transmits the data to a resourceful edge-fog node for advanced DL-based visual processing. The EFCam also applies the deep reinforcement learning (DRL) to adapt the camera configuration to maintain the desired visual sensing performance with the minimum energy consumption under dynamic variations of industrial application requirement and wireless channel conditions at the factory. The detailed design of EFCam can be found in our IEEE SECON’21 paper<a href="https://github.com/tanrui/www/blob/master/pub/EFCam-SECON21.pdf" target="_blank" rel="noopener">(PDF)</a> and its extended version in TMC<a href="https://tanrui.github.io/pub/EFCam-TMC.pdf" target="_blank" rel="noopener">(PDF)</a>.</p>
<p>We have applied EFCam to improve the quality control of the ink cartridge manufacturing lines at the factories of Hewlett-Packard (HP) Inc. Our target application is HP’s ink extraction testing (IET), the ﬁnal quality control procedure used to detect any defective batch in which the ink cartridges’ performance deviates from the speciﬁcation. The IET machine controls a stepper motor pump to extract ink from the cartridge and uses a liquid pressure sensor continuously measures the pressure in the tube. The measured pressure profile is assessed to determine the performance of the tested cartridge. However, formation of air bubbles with a sufﬁciently large volume can affect the liquid pressure measurement and cause many false alarms.</p>
<div style="margin-top: 3rem;"></div>
<img src="/posts/research/images/sensing/fig_iet.png"
    
    
    
    
    
        class="center"
    
>

<div style="margin-top: 3rem;"></div>
<p>To reduce the recall rate in detecting the defective cartridges, we apply EFCam to monitor the air bubbles at the Y-joint of tube. The camera applies a two-step image processing pipeline. The first step runs the convolutional neural network to detect the presence of the air bubbles. The second step measures the volume of the detected air bubbles using a computer vision (CV)-based image processing framework. The details of this application can be found in our IEEE SECON’21 paper<a href="https://tanrui.github.io/pub/IET-SECON21.pdf" target="_blank" rel="noopener">(PDF)</a>.</p>
<h3 id="rescuing-speech-recognition-from-microphone-heterogeneity">Rescuing speech recognition from microphone heterogeneity</h3>
<div style="margin-top: 3rem;"></div>
<img src="/posts/research/images/sensing/phyaug.png"
    
    
    
    
    
        class="center"
    
>

<div style="margin-top: 3rem;"></div>
<p>Run-time domain shifts from training-phase domains are common in sensing systems designed with deep learning. The shifts can be caused by sensor characteristic variations and/or discrepancies between the design-phase model and the actual model of the sensed physical process. In the cyber-physical sensing applications, both the monitored physical processes and the sensing apparatus are often governed by certain first principles. In this paper, we investigate the approach to exploit such first principles as a form of prior knowledge to reduce the demand on target-domain data for model transfer. We propose a new approach called physics-directed data augmentation (PhyAug). Specifically, we use a minimum amount of data collected from the target domain to estimate the parameters of the first principle governing the domain shift process and then use the parametric first principle to generate augmented target-domain training data. Finally, the augmented target-domain data samples are used to transfer or retrain the source-domain DNN.</p>
<p>In this paper, we apply PhyAug two acoustic sensing applications and quantify the performance gains compared with other transfer learning approaches. The first and the second case studies aim to adapt DNNs for keyword spotting (KWS) and automatic speech recognition (ASR) respectively to individual deployed microphones. The domain shifts are mainly from microphone’s hardware characteristics. Our tests show that the microphone can lead to 15% to 35% absolute accuracy drops, depending on the microphone quality. Instead of collecting training data using the target microphone, PhyAug uses a smartphone to play a 5-second white noise and then estimates the frequency response curve of the microphone based on its received noise data. Then, using the estimated curve, the existing samples in the factory training dataset are transformed into new training data samples, which are used to transfer the DNN to the target domain of the microphone by a retraining process. Experiment results show that PhyAug recovers the microphone-induced accuracy loss by 53%-72% and 37%-70% in KWS and ASR, respectively. PhyAug also outperforms the existing approaches including <a href="https://papers.nips.cc/paper/2017/file/21c5bba1dd6aed9ab48c2b34c1a0adde-Paper.pdf" target="_blank" rel="noopener">FADA</a> that is a domain adaptation approach based on adversarial learning and <a href="https://arxiv.org/pdf/2003.12425.pdf" target="_blank" rel="noopener">Mic2Mic</a> and <a href="https://dl.acm.org/doi/10.1109/IPSN.2018.00048" target="_blank" rel="noopener">CDA</a> that are designed specifically to address microphone heterogeneity. Note that KWS and ASR diﬀer significantly in DNN model depth and complexity. Specifically, we use DeepSpeech2 ASR model, it has 86.6 million weights, which is 175 times larger than the KWS CNN in terms of the weight amount.</p>
<div style="margin-top: 3rem;"></div>
<img src="/posts/research/images/sensing/phy.png"
    
    
    
    
    
        class="center"
    
>

<div style="margin-top: 3rem;"></div>
<h3 id="bibliography">Bibliography</h3>
<h4 id="our-research">Our research</h4>
<p>[1] Quality-driven Volcanic Earthquake Detection using Wireless Sensor Networks. Rui Tan, Guoliang Xing, Jinzhu Chen, Wen-Zhan Song, Renjie Huang. <em>The 31st IEEE Real-Time Systems Symposium (RTSS)</em>, pp. 271-280, Nov 30 - Dec 3, 2010, San Diego, CA, USA.
[2] Volcanic Earthquake Timing using Wireless Sensor Networks. Guojin Liu, Rui Tan; Ruogu Zhou; Guoliang Xing; Wen-Zhan Song; Jonathan M. Lees. <em>The 12th ACM/IEEE Conference on Information Processing in Sensor Networks (IPSN)</em>, April 8-11, 2013, Philadelphia, PA, USA. CPS Week 2013. (The first two authors are listed in alphabetic order.)
[3] ORBIT: A Smartphone-Based Platform for Data-Intensive Embedded Sensing Applications. Mohammad-Mahdi Moazzami, Dennis E. Phillips, Rui Tan, Guoliang Xing. <em>IEEE Transactions on Mobile Computing (TMC)</em>. Vol. 16, No. 3, pp. 801-815, March 2017.
[4] Aquatic Debris Monitoring Using Smartphone-Based Robotic Sensors. Yu Wang, Rui Tan, Guoliang Xing, Jianxun Wang, Xiaobo Tan, Xiaoming Liu, Xiangmao Chang. <em>The 13th ACM/IEEE International Conference on Information Processing in Sensor Networks (IPSN)</em>, April 15-17, 2014, Berlin, Germany.
[5] Samba: A Smartphone-Based Robot System for Energy-Efficient Aquatic Environment Monitoring. Yu Wang, Rui Tan, Guoliang Xing, Jianxun Wang, Xiaobo Tan, Xiaoming Liu. <em>The 14th ACM/IEEE International Conference on Information Processing in Sensor Networks (IPSN)</em>, April 13-17, 2015, Seattle, WA, USA.
[6] Deep Room Recognition Using Inaudible Echos. Qun Song, Chaojie Gu, Rui Tan. <em>Proceedings of the ACM on Interactive, Mobile, Wearable and Ubiquitous Technologies (IMWUT)</em>. <em>The ACM International Joint Conference on Pervasive and Ubiquitous Computing (Ubicomp)</em>, October 8-12, 2018, Singapore.
[7] EFCam: Configuration-Adaptive Fog-Assisted Wireless Cameras with Reinforcement Learning. Siyuan Zhou, Duc Van Le, Joy Qiping Yang, Rui Tan, Daren Ho. <em>International Conference on Sensing, Communication and Networking (SECON)</em>, July 6-9, 2021, held online.
[8] Configuration-Adaptive Wireless Visual Sensing System with Deep Reinforcement Learning. Siyuan Zhou, Duc Van Le, Rui Tan, Joy Qiping Yang, Daren Ho. <em>IEEE Transactions on Mobile Computing (TMC)</em>
[9] Improving Quality Control with Industrial AIoT at HP Factories: Experiences and Learned Lessons. Joy Qiping Yang, Siyuan Zhou; Duc Van Le; Daren Ho; Rui Tan. <em>International Conference on Sensing, Communication and Networking (SECON)</em>, July 6-9, 2021, held online.</p>

        </div>

        
        <div class="row pl-3 pr-3">
        
        <div class="col-md-6 share-buttons">
        
            <strong>Share on:</strong>
            
            <a class="btn btn-sm facebook-btn" href="https://www.facebook.com/sharer.php?u=https%3a%2f%2frrwang1.github.io%2fposts%2fresearch%2fiot-sensing%2facoustic%2f" target="_blank">
              <i class="fab fa-facebook"></i>
            </a>
            
            
                <a class="btn btn-sm twitter-btn" href="https://twitter.com/share?url=https%3a%2f%2frrwang1.github.io%2fposts%2fresearch%2fiot-sensing%2facoustic%2f&text=Acoustic%20and%20imagery%20sensing&via=NTU%20IoT%20Research%20Group" target="_blank">
                  <i class="fab fa-twitter"></i>
                </a>
            
            
            
            
            
                <a class="btn btn-sm linkedin-btn" href="https://www.linkedin.com/shareArticle?url=https%3a%2f%2frrwang1.github.io%2fposts%2fresearch%2fiot-sensing%2facoustic%2f&title=Acoustic%20and%20imagery%20sensing" target="_blank">
                  <i class="fab fa-linkedin"></i>
                </a>
            
            
             
            
                 <a class="btn btn-sm whatsapp-btn" href="https://api.whatsapp.com/send?text=Acoustic%20and%20imagery%20sensing https%3a%2f%2frrwang1.github.io%2fposts%2fresearch%2fiot-sensing%2facoustic%2f" target="_blank">
                  <i class="fab fa-whatsapp"></i>
                </a>
            
            
                <a class="btn btn-sm email-btn" href="mailto:?subject=Acoustic%20and%20imagery%20sensing&body=https%3a%2f%2frrwang1.github.io%2fposts%2fresearch%2fiot-sensing%2facoustic%2f" target="_blank">
                  <i class="fas fa-envelope-open-text"></i>
                </a>
            
          
          </div>

        
        
        </div>



      
      <hr />
        







  





  
  

  
  

  
  

  
  

  
  

  
  

  
    
    
  
  

  
  

  
  

  
  

  
  

  
  

  
  

  
  

  
  

  
  

  
  

  
  

  
  

  
  

  
  

  
  

  
  

  
  

  
  

  
  

  
  

  
  

  
  

  
  

  
  

  
  

  
  

  
  

  
  

  
  

  
  

  
  

  
  

  
  

  
  

  
  

  
  

  
  

  
  

  
  

  
  

  
  

  
  

  
  


<div class="row next-prev-navigator">
  
    <div class="col-md-6 previous-article">
      <a href="/posts/research/iot-sensing/building/" title="Building sensing" class="btn btn-outline-info">
        <div><i class="fas fa-chevron-circle-left"></i> Prev</div>
        <div class="next-prev-text">Building sensing</div>
      </a>
    </div>
  
  
      
      
        
      
      <div class="col-md-6 next-article">
        <a href="/posts/members/faculty/" title="Faculty" class="btn btn-outline-info">
          <div>Next <i class="fas fa-chevron-circle-right"></i></div>
          <div class="next-prev-text">Faculty</div>
        </a>
      </div>
    
</div>

      <hr />

      
      

      
      

      </div>
    </div>
  </div>
  
  <a id="scroll-to-top" class="btn"><i class="fas fa-chevron-circle-up"></i></a>
  
</section>


      
      
  <section class="toc-section" id="toc-section">
    
  </section>

    </div>

    
    












  
  
    
  

  
  
    
  

  
  

  
  
    
    
      
    
  


  
  
  

  
  
  

  
  
  
    
  
  

  
  
  

  <footer id="footer" class="container-fluid text-center align-content-center footer pb-2">
    <div class="container pt-5">
      <div class="row text-left">
        
        
        
        
      </div>
    </div>
    
    
    <hr />
    <div class="container">
      <div class="row text-left">
        <div class="col-md-4">
          <a id="theme" href="https://github.com/hugo-toha/toha" target="_blank" rel="noopener">
            <img src="/images/theme-logo_hu8376fd15465fef26ffe66b6bcf0ca686_13669_32x0_resize_box_3.png" alt="Toha Theme Logo">
            Toha
          </a>
        </div>
        <div class="col-md-4 text-center"><sub><sup>NTU IoT Research Group Copyright © 2021. All rights reserved. <br /> Webmasters are Chaojie Gu and Zhenyu Yan (06/2020-07/2021);  <br /> Dongfang Guo and Wenjie Luo (07/2021-06/2023); Rongrong Wang (07/2023-).</sup></sub></div>
        <div class="col-md-4 text-right">
          <a id="hugo" href="https://gohugo.io/" target="_blank" rel="noopener">Powered by
          <img
            src="/images/hugo-logo.svg"
            alt="Hugo Logo"
            height="18"
          />
          </a>
        </div>
      </div>
    </div>
    
  </footer>


    <script src="/application.91f59729c47e36271f8f97bcd56f93957b4751fd44d43fa7b73ff6354ea73821.js" integrity="sha256-kfWXKcR&#43;Nicfj5e81W&#43;TlXtHUf1E1D&#43;ntz/2NU6nOCE=" defer></script>



    
     

    
    

</body>
</html>
